{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25753,
     "status": "ok",
     "timestamp": 1754770885630,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "8LfDfVSSCgJg",
    "outputId": "8b7235c9-e393-4ef6-8ba8-6a7d43ae3d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/886 Project/nlg-xid\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/886 Project/nlg-xid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsarQS6ICqPz"
   },
   "source": [
    "## 1. Evaluate Model Performance (both train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnSbv0vFCxDF"
   },
   "source": [
    "UNSW-NB15 Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22583,
     "status": "ok",
     "timestamp": 1754732274079,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "wJOM2oxvGU1-",
    "outputId": "48bfbcea-874f-4842-b54b-a014ccbe629e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/unsw_nb15/nb15_train.csv\n",
      "Loaded 175341 test samples\n",
      "Testing AE model...\n",
      "[6 1 0 4 8 7 3 2 9 5]\n",
      "Test Accuracy: 0.8739\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.25      0.36      2000\n",
      "           1       0.66      0.12      0.21      1746\n",
      "           2       0.50      0.11      0.18     12264\n",
      "           3       0.63      0.94      0.76     33393\n",
      "           4       0.93      0.89      0.91     18184\n",
      "           5       1.00      0.98      0.99     40000\n",
      "           6       1.00      1.00      1.00     56000\n",
      "           7       0.93      0.73      0.82     10491\n",
      "           8       0.66      0.63      0.65      1133\n",
      "           9       0.91      0.15      0.26       130\n",
      "\n",
      "    accuracy                           0.87    175341\n",
      "   macro avg       0.79      0.58      0.61    175341\n",
      "weighted avg       0.87      0.87      0.86    175341\n",
      "\n",
      "Test Error: 0.1261\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nb15 --train_path data/unsw_nb15/nb15_train.csv --test_path data/unsw_nb15/nb15_train.csv --model_type ae --model_path trained_models/nb15_ae_model.pkl --ae_weights trained_models/nb15_ae_model_autoencoder.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5824,
     "status": "ok",
     "timestamp": 1754732279908,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "kTX7HdBAGfUR",
    "outputId": "8cd53758-d417-4393-9056-3e95635b10e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/unsw_nb15/nb15_test.csv\n",
      "Loaded 82332 test samples\n",
      "Testing AE model...\n",
      "[6 7 1 2 3 0 4 9 8 5]\n",
      "Test Accuracy: 0.7299\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.14      0.08       677\n",
      "           1       0.03      0.11      0.05       583\n",
      "           2       0.20      0.04      0.06      4089\n",
      "           3       0.61      0.77      0.68     11132\n",
      "           4       0.61      0.61      0.61      6062\n",
      "           5       1.00      0.45      0.62     18871\n",
      "           6       0.87      1.00      0.93     37000\n",
      "           7       0.30      0.53      0.39      3496\n",
      "           8       0.29      0.41      0.34       378\n",
      "           9       0.60      0.07      0.12        44\n",
      "\n",
      "    accuracy                           0.73     82332\n",
      "   macro avg       0.46      0.41      0.39     82332\n",
      "weighted avg       0.77      0.73      0.72     82332\n",
      "\n",
      "Test Error: 0.2701\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nb15 --train_path data/unsw_nb15/nb15_train.csv --test_path data/unsw_nb15/nb15_test.csv --model_type ae --model_path trained_models/nb15_ae_model.pkl --ae_weights trained_models/nb15_ae_model_autoencoder.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NriPWKb0C2xv"
   },
   "source": [
    "NSL-KDD Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6765,
     "status": "ok",
     "timestamp": 1754732286678,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "kxW7WlzCGVd0",
    "outputId": "4228fb82-9c19-45f9-8956-c81f8f4cb329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/nsl_kdd/KDDTrain+.txt\n",
      "Loaded 125973 test samples\n",
      "Testing AE model...\n",
      "[4 0 2 1 3]\n",
      "Test Accuracy: 0.9978\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45927\n",
      "           1       1.00      1.00      1.00     11656\n",
      "           2       0.96      0.94      0.95       995\n",
      "           3       0.93      0.73      0.82        52\n",
      "           4       1.00      1.00      1.00     67343\n",
      "\n",
      "    accuracy                           1.00    125973\n",
      "   macro avg       0.98      0.93      0.95    125973\n",
      "weighted avg       1.00      1.00      1.00    125973\n",
      "\n",
      "Test Error: 0.0022\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nsl-kdd --train_path data/nsl_kdd/KDDTrain+.txt --test_path data/nsl_kdd/KDDTrain+.txt --model_type ae --model_path trained_models/nsl-kdd_ae_model.pkl --ae_weights trained_models/nsl-kdd_ae_model_autoencoder.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4684,
     "status": "ok",
     "timestamp": 1754732291360,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "-cEweFWeG3NN",
    "outputId": "214dac39-8fc8-4cfe-edf1-90282eb78dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/nsl_kdd/KDDTest+.txt\n",
      "Loaded 22544 test samples\n",
      "Testing AE model...\n",
      "[0 4 1 2 3]\n",
      "Test Accuracy: 0.7748\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88      7458\n",
      "           1       0.85      0.71      0.78      2421\n",
      "           2       0.86      0.11      0.19      2887\n",
      "           3       0.31      0.34      0.32        67\n",
      "           4       0.69      0.93      0.79      9711\n",
      "\n",
      "    accuracy                           0.77     22544\n",
      "   macro avg       0.72      0.59      0.59     22544\n",
      "weighted avg       0.80      0.77      0.74     22544\n",
      "\n",
      "Test Error: 0.2252\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nsl-kdd --train_path data/nsl_kdd/KDDTrain+.txt --test_path data/nsl_kdd/KDDTest+.txt --model_type ae --model_path trained_models/nsl-kdd_ae_model.pkl --ae_weights trained_models/nsl-kdd_ae_model_autoencoder.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIn1lbMSC4pB"
   },
   "source": [
    "UNSW-NB15 XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17473,
     "status": "ok",
     "timestamp": 1754732308835,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "Lpc0HltcGV2B",
    "outputId": "5f8c3a4a-7001-48d7-a025-123418228610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/unsw_nb15/nb15_train.csv\n",
      "Loaded 175341 test samples\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/test_model.py:106: UserWarning: [09:38:16] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Testing XGB model...\n",
      "[6 1 0 4 8 7 3 2 9 5]\n",
      "Test Accuracy: 0.8970\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.30      0.45      2000\n",
      "           1       0.86      0.27      0.41      1746\n",
      "           2       0.50      0.31      0.38     12264\n",
      "           3       0.70      0.92      0.79     33393\n",
      "           4       0.98      0.91      0.94     18184\n",
      "           5       1.00      0.99      1.00     40000\n",
      "           6       1.00      1.00      1.00     56000\n",
      "           7       0.96      0.79      0.87     10491\n",
      "           8       0.84      0.94      0.89      1133\n",
      "           9       0.98      1.00      0.99       130\n",
      "\n",
      "    accuracy                           0.90    175341\n",
      "   macro avg       0.88      0.74      0.77    175341\n",
      "weighted avg       0.90      0.90      0.89    175341\n",
      "\n",
      "Test Error: 0.1030\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nb15 --train_path data/unsw_nb15/nb15_train.csv --test_path data/unsw_nb15/nb15_train.csv --model_type xgb --model_path trained_models/nb15_xgb_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10545,
     "status": "ok",
     "timestamp": 1754732319378,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "Y2wwb0PDHBOw",
    "outputId": "d191aa5c-720b-4278-bc43-61cd64930695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/unsw_nb15/nb15_test.csv\n",
      "Loaded 82332 test samples\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/test_model.py:106: UserWarning: [09:38:33] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Testing XGB model...\n",
      "[6 7 1 2 3 0 4 9 8 5]\n",
      "Test Accuracy: 0.7706\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.01      0.01       677\n",
      "           1       0.03      0.19      0.05       583\n",
      "           2       0.16      0.06      0.09      4089\n",
      "           3       0.45      0.66      0.53     11132\n",
      "           4       0.73      0.74      0.73      6062\n",
      "           5       1.00      0.58      0.74     18871\n",
      "           6       1.00      1.00      1.00     37000\n",
      "           7       0.82      0.84      0.83      3496\n",
      "           8       0.42      0.83      0.56       378\n",
      "           9       0.44      0.70      0.54        44\n",
      "\n",
      "    accuracy                           0.77     82332\n",
      "   macro avg       0.50      0.56      0.51     82332\n",
      "weighted avg       0.84      0.77      0.79     82332\n",
      "\n",
      "Test Error: 0.2294\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nb15 --train_path data/unsw_nb15/nb15_train.csv --test_path data/unsw_nb15/nb15_test.csv --model_type xgb --model_path trained_models/nb15_xgb_model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXxhkzpmC7MS"
   },
   "source": [
    "NSL-KDD XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9916,
     "status": "ok",
     "timestamp": 1754732329304,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "UL-M-qF4GWT2",
    "outputId": "7c30f8ce-d189-4e40-917b-aa8d0c9e77a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/nsl_kdd/KDDTrain+.txt\n",
      "Loaded 125973 test samples\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/test_model.py:106: UserWarning: [09:38:43] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Testing XGB model...\n",
      "[4 0 2 1 3]\n",
      "Test Accuracy: 0.9999\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     45927\n",
      "           1       1.00      1.00      1.00     11656\n",
      "           2       1.00      1.00      1.00       995\n",
      "           3       1.00      0.98      0.99        52\n",
      "           4       1.00      1.00      1.00     67343\n",
      "\n",
      "    accuracy                           1.00    125973\n",
      "   macro avg       1.00      1.00      1.00    125973\n",
      "weighted avg       1.00      1.00      1.00    125973\n",
      "\n",
      "Test Error: 0.0001\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nsl-kdd --train_path data/nsl_kdd/KDDTrain+.txt --test_path data/nsl_kdd/KDDTrain+.txt --model_type xgb --model_path trained_models/nsl-kdd_xgb_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4918,
     "status": "ok",
     "timestamp": 1754732334227,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "fh-gLYo5Hej8",
    "outputId": "a375cc6b-d472-4bf6-d9bb-5d6e82f65ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/nsl_kdd/KDDTest+.txt\n",
      "Loaded 22544 test samples\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/test_model.py:106: UserWarning: [09:38:52] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Testing XGB model...\n",
      "[0 4 1 2 3]\n",
      "Test Accuracy: 0.7743\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.84      0.90      7458\n",
      "           1       0.83      0.64      0.72      2421\n",
      "           2       0.98      0.06      0.12      2887\n",
      "           3       0.67      0.09      0.16        67\n",
      "           4       0.68      0.97      0.80      9711\n",
      "\n",
      "    accuracy                           0.77     22544\n",
      "   macro avg       0.82      0.52      0.54     22544\n",
      "weighted avg       0.83      0.77      0.73     22544\n",
      "\n",
      "Test Error: 0.2257\n"
     ]
    }
   ],
   "source": [
    "!python3 eval.py --dataset nsl-kdd --train_path data/nsl_kdd/KDDTrain+.txt --test_path data/nsl_kdd/KDDTest+.txt --model_type xgb --model_path trained_models/nsl-kdd_xgb_model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra2OIcTDC_Pq"
   },
   "source": [
    "## 2. NB15 Experiments with Autoencoder on Train Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2151674,
     "status": "ok",
     "timestamp": 1754745173008,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "XpQiNu5KEJvO",
    "outputId": "8a6aaa28-6962-4c9f-a3fd-301919e96186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir ablation_nb15_ae_train --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:22<00:00,  8.93it/s]\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to ablation_nb15_ae_train:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.315\n",
      "  bleu: 0.0365180832851272\n",
      "  shap_fidelity: 0.97\n",
      "  judge_overall: 2.539\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir ablation_nb15_ae_train --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:22<00:00,  8.96it/s]\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to ablation_nb15_ae_train:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.004839093117747631\n",
      "  shap_fidelity: 0.665\n",
      "  judge_overall: 3.598\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir ablation_nb15_ae_train --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:21<00:00,  9.11it/s]\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to ablation_nb15_ae_train:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.002489244967000503\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 2.422\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: ablation_nb15_ae_train\n",
      "Summary saved to: ablation_nb15_ae_train/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nb15_ae_model.pkl \\\n",
    "    --test_path data/unsw_nb15/nb15_train_200.csv \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_nb15_ae_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EDyz0jNDrpY"
   },
   "source": [
    "## 3. NB15 Experiments with Autoencoder on Test Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2242138,
     "status": "ok",
     "timestamp": 1754738416628,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "LTK96REvE-Ue",
    "outputId": "11574092-1118-4e9a-a441-922208c4dcaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_ae_test --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:22<00:00,  9.09it/s]\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_ae_test:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.53\n",
      "  bleu: 0.045077688390813185\n",
      "  shap_fidelity: 0.955\n",
      "  judge_overall: 2.643\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_ae_test --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:21<00:00,  9.24it/s]\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_ae_test:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.013312701526103341\n",
      "  shap_fidelity: 0.57\n",
      "  judge_overall: 3.678\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_ae_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_ae_test --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nb15_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:21<00:00,  9.33it/s]\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_ae_test:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.026522583941353105\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 2.447\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_nb15_ae_test\n",
      "Summary saved to: results/ablation_nb15_ae_test/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nb15_ae_model.pkl \\\n",
    "    --test_path data/unsw_nb15/nb15_test_200.csv \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_nb15_ae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFPkNOHyDw-e"
   },
   "source": [
    "## 4. NB15 Experiments with XGB on Train Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2201011,
     "status": "ok",
     "timestamp": 1754740617641,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "sD2oqIDDFJb7",
    "outputId": "7f013925-7edb-4d2c-c479-bcad3daeda67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_train --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [11:20:23] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 11:20:25.628837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754738425.645502   27530 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754738425.650324   27530 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754738425.664370   27530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738425.664392   27530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738425.664395   27530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738425.664399   27530 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 11:20:25.668238: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_train:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.42\n",
      "  bleu: 0.04426746001401173\n",
      "  shap_fidelity: 0.995\n",
      "  judge_overall: 2.768\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_train --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [11:26:14] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 11:26:15.044137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754738775.063246   29049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754738775.069069   29049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754738775.083980   29049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738775.084012   29049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738775.084016   29049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754738775.084021   29049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 11:26:15.088490: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_train:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.007784455558514052\n",
      "  shap_fidelity: 0.93\n",
      "  judge_overall: 3.197\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_train_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_train --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [11:51:54] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_train_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 11:51:55.067562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754740315.087924   35675 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754740315.094546   35675 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754740315.110040   35675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740315.110074   35675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740315.110078   35675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740315.110081   35675 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 11:51:55.115125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_train:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.009208337728211868\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.354\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_nb15_xgb_train\n",
      "Summary saved to: results/ablation_nb15_xgb_train/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nb15_xgb_model.pkl \\\n",
    "    --test_path data/unsw_nb15/nb15_train_200.csv \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_nb15_xgb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QftfbIjXD0Uh"
   },
   "source": [
    "## 5. NB15 Experiments with XGB on Test Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2305503,
     "status": "ok",
     "timestamp": 1754742923147,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "lB8kD4qwFPmF",
    "outputId": "dc86dedd-e8ff-4ad3-a2cf-6c08f53eaa41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_test --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [11:57:04] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 11:57:05.795185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754740625.815341   37027 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754740625.821391   37027 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754740625.836745   37027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740625.836772   37027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740625.836776   37027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740625.836779   37027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 11:57:05.841513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_test:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.53\n",
      "  bleu: 0.02832641663531789\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 2.917\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_test --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [12:03:09] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 12:03:10.441000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754740990.460424   38609 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754740990.466488   38609 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754740990.482149   38609 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740990.482176   38609 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740990.482180   38609 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754740990.482185   38609 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 12:03:10.486838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_test:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.009480386369369593\n",
      "  shap_fidelity: 0.925\n",
      "  judge_overall: 3.206\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nb15_xgb_model.pkl --test_path data/unsw_nb15/nb15_test_200.csv --config configs/defaults.yaml --output_dir results/ablation_nb15_xgb_test --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nb15_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [12:30:08] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nb15 dataset\n",
      "Original training samples: 175341\n",
      "Loading test data from data/unsw_nb15/nb15_test_200.csv\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 12:30:09.666424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754742609.685843   45569 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754742609.691791   45569 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754742609.707821   45569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754742609.707859   45569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754742609.707863   45569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754742609.707866   45569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 12:30:09.712759: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_nb15_xgb_test:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.010246702520691363\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.371\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_nb15_xgb_test\n",
      "Summary saved to: results/ablation_nb15_xgb_test/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nb15_xgb_model.pkl \\\n",
    "    --test_path data/unsw_nb15/nb15_test_200.csv \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_nb15_xgb_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRW1ihGmEBmc"
   },
   "source": [
    "## 6. KDD Experiments with Autoencoder on Train Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2270822,
     "status": "ok",
     "timestamp": 1754773165972,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "_wvaLTNwFSgY",
    "outputId": "77fec38a-ce2c-4d8f-def7-e221e2b036da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_train --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:15<00:00, 13.10it/s]\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_ae_train:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.935\n",
      "  bleu: 0.027428997507595563\n",
      "  shap_fidelity: 0.97\n",
      "  judge_overall: 2.666\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_train --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:15<00:00, 13.28it/s]\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "OpenRouter generation failed: 503 Server Error: Service Unavailable for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Results saved to results/ablation_kdd_ae_train:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.01273151588387193\n",
      "  shap_fidelity: 0.235\n",
      "  judge_overall: 3.61\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_train --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:14<00:00, 13.62it/s]\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "OpenRouter generation failed: 503 Server Error: Service Unavailable for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Results saved to results/ablation_kdd_ae_train:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.017992647351403948\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.131\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_kdd_ae_train\n",
      "Summary saved to: results/ablation_kdd_ae_train/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nsl-kdd_ae_model.pkl \\\n",
    "    --test_path data/nsl_kdd/KDDTrain+200.txt \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_kdd_ae_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY8xdMoYEBmc"
   },
   "source": [
    "## 7. KDD Experiments with Autoencoder on Test Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2293083,
     "status": "ok",
     "timestamp": 1754775459056,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "BH-UzSFvFcer",
    "outputId": "94917618-8adb-4fc5-e02c-17a6816dfcc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_test --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:14<00:00, 13.49it/s]\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_ae_test:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.93\n",
      "  bleu: 0.026078533208856263\n",
      "  shap_fidelity: 0.96\n",
      "  judge_overall: 2.82\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_test --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:14<00:00, 13.41it/s]\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_ae_test:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.01480815901493103\n",
      "  shap_fidelity: 0.22\n",
      "  judge_overall: 3.743\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_ae_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_ae_test --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_ae_model.pkl\n",
      "Loading PyTorch autoencoder weights from trained_models/nsl-kdd_ae_model_autoencoder.pth\n",
      "Loaded AE model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100% 200/200 [00:15<00:00, 13.29it/s]\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_ae_test:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.015968478827228575\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.174\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_kdd_ae_test\n",
      "Summary saved to: results/ablation_kdd_ae_test/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nsl-kdd_ae_model.pkl \\\n",
    "    --test_path data/nsl_kdd/KDDTest+200.txt \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_kdd_ae_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGZw1G2mEBmc"
   },
   "source": [
    "## 8. KDD Experiments with XGB on Train Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2216832,
     "status": "ok",
     "timestamp": 1754777675890,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "mMThRXI6Fglf",
    "outputId": "8f735eb9-568b-4a82-bb86-0f2f5ad8f24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_train --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [21:37:48] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 21:37:50.632821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754775470.652858   20583 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754775470.659279   20583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754775470.676832   20583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775470.676858   20583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775470.676862   20583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775470.676866   20583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 21:37:50.682325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_xgb_train:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.985\n",
      "  bleu: 0.028410633550569744\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.182\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_train --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [21:42:47] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 21:42:48.659291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754775768.679089   21831 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754775768.685169   21831 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754775768.701011   21831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775768.701043   21831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775768.701047   21831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754775768.701051   21831 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 21:42:48.705644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "OpenRouter generation failed: 503 Server Error: Service Unavailable for url: https://openrouter.ai/api/v1/chat/completions\n",
      "OpenRouter generation failed: 503 Server Error: Service Unavailable for url: https://openrouter.ai/api/v1/chat/completions\n",
      "Results saved to results/ablation_kdd_xgb_train:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.006328030109527785\n",
      "  shap_fidelity: 0.46\n",
      "  judge_overall: 3.701\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTrain+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_train --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [22:09:35] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTrain+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 22:09:36.528272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754777376.548269   28439 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754777376.554142   28439 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754777376.569180   28439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777376.569213   28439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777376.569216   28439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777376.569220   28439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 22:09:36.574559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_xgb_train:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.0018825209279909906\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.132\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_kdd_xgb_train\n",
      "Summary saved to: results/ablation_kdd_xgb_train/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nsl-kdd_xgb_model.pkl \\\n",
    "    --test_path data/nsl_kdd/KDDTrain+200.txt \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_kdd_xgb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW55xYqgEBmd"
   },
   "source": [
    "## 9. KDD Experiments with XGB on Test Set (200 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2188219,
     "status": "ok",
     "timestamp": 1754779864116,
     "user": {
      "displayName": "Fatima Sohail",
      "userId": "18373063643267229168"
     },
     "user_tz": 240
    },
    "id": "97z-XB6YFhCJ",
    "outputId": "992cae1e-0ccb-4a62-f100-212bf1ae76e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: Experiment 1: rules\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_test --api_key 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 --explain rules --experiment_name exp_1_rules\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [22:14:41] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 22:14:42.715228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754777682.734860   29713 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754777682.740882   29713 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754777682.756132   29713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777682.756157   29713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777682.756161   29713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777682.756166   29713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 22:14:42.761549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating rules explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_xgb_test:\n",
      "  - Explanations: exp_1_rules_explanations.json\n",
      "  - Metrics: exp_1_rules_metrics.json\n",
      "  - Judge scores: exp_1_rules_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  rule_coverage: 0.975\n",
      "  bleu: 0.025712997002451922\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.039\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 1: rules\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_test --api_key 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 --explain llm --llm_provider openrouter --ablation_llm_inputs full --experiment_name exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct --llm_model meta-llama/llama-3.1-8b-instruct --api_key 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\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [22:19:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 22:19:43.473993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754777983.493029   30954 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754777983.498898   30954 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754777983.513961   30954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777983.513990   30954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777983.513994   30954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754777983.513998   30954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 22:19:43.518778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating llm explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_xgb_test:\n",
      "  - Explanations: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_explanations.json\n",
      "  - Metrics: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_metrics.json\n",
      "  - Judge scores: exp_2_llm_openrouter_full_meta_llama_llama_3.1_8b_instruct_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.005608623768101072\n",
      "  shap_fidelity: 0.475\n",
      "  judge_overall: 3.615\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 2: llm with openrouter provider, full inputs, model meta-llama/llama-3.1-8b-instruct\n",
      "\n",
      "============================================================\n",
      "Running: Experiment 3: shap_only\n",
      "Command: python pipeline/eval_model.py --model_path trained_models/nsl-kdd_xgb_model.pkl --test_path data/nsl_kdd/KDDTest+200.txt --config configs/defaults.yaml --output_dir results/ablation_kdd_xgb_test --api_key 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 --explain shap_only --experiment_name exp_3_shap_only\n",
      "============================================================\n",
      "Loading trained model from trained_models/nsl-kdd_xgb_model.pkl\n",
      "/content/drive/MyDrive/886 Project/nlg-xid/pipeline/eval_model.py:40: UserWarning: [22:46:15] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  pipe = pickle.load(f)\n",
      "Loaded XGB model trained on nsl-kdd dataset\n",
      "Original training samples: 125973\n",
      "Loading test data from data/nsl_kdd/KDDTest+200.txt\n",
      "Loaded 200 test samples\n",
      "Running inference and generating explanations...\n",
      "2025-08-09 22:46:16.950663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754779576.972799   37506 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754779576.978819   37506 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754779576.995977   37506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754779576.996004   37506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754779576.996008   37506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754779576.996012   37506 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 22:46:17.000821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Generating shap_only explanations...\n",
      "Computing evaluation metrics...\n",
      "Running LLM-as-judge evaluation...\n",
      "Results saved to results/ablation_kdd_xgb_test:\n",
      "  - Explanations: exp_3_shap_only_explanations.json\n",
      "  - Metrics: exp_3_shap_only_metrics.json\n",
      "  - Judge scores: exp_3_shap_only_judge.json\n",
      "\n",
      "Evaluation Summary:\n",
      "  bleu: 0.001922757554228143\n",
      "  shap_fidelity: 1.0\n",
      "  judge_overall: 3.05\n",
      "Evaluation completed successfully!\n",
      "✓ Completed: Experiment 3: shap_only\n",
      "\n",
      "============================================================\n",
      "ABLATION STUDY COMPLETED\n",
      "============================================================\n",
      "Total experiments: 3\n",
      "Successful: 3\n",
      "Failed: 0\n",
      "Results saved in: results/ablation_kdd_xgb_test\n",
      "Summary saved to: results/ablation_kdd_xgb_test/ablation_summary.json\n"
     ]
    }
   ],
   "source": [
    "!python3 pipeline/run_ablations.py \\\n",
    "    --model_path trained_models/nsl-kdd_xgb_model.pkl \\\n",
    "    --test_path data/nsl_kdd/KDDTest+200.txt \\\n",
    "    --api_key [api_key] \\\n",
    "    --llm_provider openrouter \\\n",
    "    --llm_model meta-llama/llama-3.1-8b-instruct \\\n",
    "    --explain_methods rules llm shap_only\\\n",
    "    --output_dir results/ablation_kdd_xgb_test"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNLUjdeF4fkoF3/IirrhMaR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
